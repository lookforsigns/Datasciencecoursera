{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/smart_open/ssh.py:34: UserWarning: paramiko missing, opening SSH/SCP/SFTP paths will be disabled.  `pip install paramiko` to suppress\n",
      "  warnings.warn('paramiko missing, opening SSH/SCP/SFTP paths will be disabled.  `pip install paramiko` to suppress')\n"
     ]
    }
   ],
   "source": [
    "### Marjoriikka Ylisiurua University of Helsinki 28 Sep 2019\n",
    "# CSSS summer school report:\n",
    "# for modeling:\n",
    "# get tf-idf transformations for various Suomi24-document types\n",
    "# includes superfluous code\n",
    "\n",
    "import logging\n",
    "import os\n",
    "import numpy as np\n",
    "from gensim import corpora, models, similarities, utils\n",
    "from gensim.corpora import TextCorpus, MmCorpus, Dictionary\n",
    "from collections import defaultdict\n",
    "\n",
    "from time import time\n",
    "\n",
    "import csv\n",
    "\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import nltk.data\n",
    "from nltk import snowball\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "## to use sklearn instead of gensim\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "\n",
    "## init\n",
    "\n",
    "logging.basicConfig(format=\"%(asctime)s : %(levelname)s : %(message)s\", level=logging.INFO)\n",
    "\n",
    "processedText = []\n",
    "\n",
    "sampleData = True  # kiikku: analysing sample (True) or whole data set (False)\n",
    "existingLemmaset = False  # kiikku: using existing dictionary and corpus (True) or creating from scratch (False)\n",
    "\n",
    "## init file directories\n",
    "if sampleData:\n",
    "    searchDirectorySample = (\"/Users/mry/SFI/\")\n",
    "    outcomeSampleDir = (\"/Users/mry/SFI/\")\n",
    "    outcomeGensimSampleDir = (\"/Users/mry/SFI/\")  # gensim output filepath, note only one slash / !!\n",
    "    openFileName = \"/Users/mry/SFI/sampleEconomicLemmas.csv\"  ## original csv file with lemmatized texts\n",
    "    saveFileName = \"/Users/mry/SFI/sampleEconomicDict.csv\" ## save dictionary with word ids\n",
    "else:\n",
    "    searchDirectory = (\"/Users/mry/SFI/\")\n",
    "    outcomeDir = (\"/Users/mry/SFI/\")\n",
    "    outcomeGensimDir = (\"/Users/mry/SFI/\")  # gensim outcome filepath\n",
    "    openFileName = \"/Users/mry/SFI/SampleData/headEconomy.txt\" ## original csv file with lemmatized texts\n",
    "    saveFileName = \"/Users/mry/SFI/SampleData/headEconomy.csv\" ## save dictionary with word ids\n",
    "\n",
    "    ## init some document level selection parameters\n",
    "if sampleData:\n",
    "    forumNumber = 6  # which forum to search\n",
    "    appearance = 0  # how many times a word must be present to be included in the dictionary\n",
    "    howLarge = 100  # size of document chunks\n",
    "else:\n",
    "    forumNumber = 150  # which forum to search\n",
    "    appearance = 10  # how many times a word must be present to be included in the dictionary\n",
    "    howLarge = 50000  # size of document chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## class MyCorpus(object):\n",
    "# initialize corpus with dictionary and top directory\n",
    "# then read all files in the top dir and preprocess each\n",
    "class MyCorpus(object):\n",
    "    def __init__(self, top_dir):\n",
    "        # self = corpora.MmCorpus(\"/Users/mry/SFI/corpus.mm\")\n",
    "        self.top_dir = top_dir\n",
    "        self.dictionary = Dictionary(iter_documents(top_dir))\n",
    "\n",
    "    def __iter__(self):\n",
    "        for line in iter_documents(self.top_dir):\n",
    "            # print(\"\\n line\")\n",
    "            # print(line)\n",
    "            yield preProcess(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## def preProcess(documents):\n",
    "# take a list of paragraph-strings\n",
    "# tokenize each paragraph-string, lowercase, clean stopwords\n",
    "# stem/lemmatize\n",
    "def preProcess(documents):\n",
    "    ## init\n",
    "    tokens = []\n",
    "    filtered_tokens = []\n",
    "    tokens_stopwords = []\n",
    "    tokens_stemmed = []\n",
    "\n",
    "    # remove trash from text\n",
    "    # especially consider the bot ads\n",
    "    # stopSigns = \"/p p a quot > < ( ) ! , . : ; & ? * NUOLI + [ ] ... / # '' -- -blank /a “ ” http paypal\"\n",
    "    stopSigns = \"/p p a quot > < ( ) ! , . : ; & ? * NUOLI + [ ] ... / # '' -- -blank /a “ ” this message has been removed by\"\n",
    "    stoplist = [w for w in stopSigns.split()]\n",
    "    stoplist = stoplist + (stopwords.words(\"finnish\"))\n",
    "\n",
    "    ## tokenize the sentence paragraphs & edit\n",
    "\n",
    "    tokens = [nltk.word_tokenize(el) for el in documents]  # el = string paragraph element in list\n",
    "    # print(\"\\n tokenized document\")\n",
    "    # print(tokens)\n",
    "\n",
    "    # lowercase the tokens and clean the documents of stopwords\n",
    "    for text in tokens:\n",
    "        filtered_tokens = [word.lower() for word in text if not word.lower() in stoplist]\n",
    "        # print(\"\\n tokenized lowercase text, stopwords removed\")\n",
    "        # print(sorted(filtered_tokens)) # print each tokenized document paragraph\n",
    "        tokens_stopwords.append(filtered_tokens)\n",
    "\n",
    "    # print(\"\\n cleaned texts in a list by element\")\n",
    "    # print(sorted(tokens_stopwords)) # print the whole set of tokenized texts before further filtering\n",
    "\n",
    "    # if not using lemmatized word list from Korp\n",
    "    # for each token in list, stem all remaining words\n",
    "\n",
    "    tokens_stemmed = tokens_stopwords\n",
    "\n",
    "    # print(\"\\n stemmed text\")\n",
    "    # print(sorted(tokens_stemmed))\n",
    "\n",
    "    # returned list of tokens\n",
    "    return tokens_stemmed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## def weedTokens(tokens_stemmed):\n",
    "# weed token list just to include those who appear more than APPEARANCE # of times\n",
    "# encode token strings\n",
    "def weedTokens(tokens_stemmed):\n",
    "    print(\"\\nweeding tokens, including only those that appear at least \" + str(appearance) + \" times\")\n",
    "\n",
    "    ## init\n",
    "    frequency = defaultdict(int)\n",
    "    tokens_enough = []\n",
    "\n",
    "    ## count appearance of tokens\n",
    "    for token_list in tokens_stemmed:\n",
    "        # print(\"\\n token list in tokens stemmed\")\n",
    "        # print(token_list)\n",
    "        for token in token_list:\n",
    "            if token:\n",
    "                frequency[token] += 1\n",
    "\n",
    "    # discard words that only appear APPEARANCE amount of times and return the rest\n",
    "    # encode the strings to utf-8 at the same time\n",
    "    for tokens in tokens_stemmed:\n",
    "        filtered_tokens = [bytes(token, \"utf-8\") for token in tokens if frequency[token] > appearance]\n",
    "        tokens_enough.append(filtered_tokens)\n",
    "\n",
    "    # print(tokens_enough)\n",
    "\n",
    "    return tokens_enough"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## def transformation(corpus,vec):\n",
    "# transformation NOT WORKING IF MATRIX TOO SPARSE?\n",
    "# def transformation(corpus,vec):\n",
    "# init transformation that is used to convert the corpus from one vector representation to another\n",
    "# tfIdf is simple transformation\n",
    "# it takes documents represented as bags of words counts and applies a weighting\n",
    "# which discounts common terms (promotes rare terms)\n",
    "# tfidf scales the resulting vector to unit length, in the Euclidean norm\n",
    "#    tfidf = models.TfidfModel(corpus)\n",
    "#    print(\"\\n tfidf-transformation for corpus: format - (ID, #)\")\n",
    "#    print(tfidf[vec])\n",
    "\n",
    "# transform a corpus and index it\n",
    "#    index = similarities.SparseMatrixSimilarity(tfidf[corpus],num_features=2)\n",
    "\n",
    "# query the similarity of query vector against every document in the corpus\n",
    "#    sims = index[tfidf[vec]]\n",
    "#    print(\"\\n Similarity vector: format - (sample document x corpus stem ID, similarity score %)\")\n",
    "#    print(sims)\n",
    "#    return(list(enumerate(sims)))\n",
    "\n",
    "#  -> which document is most similar to our vector, what is its score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-10-09 10:33:15,057 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-10-09 10:33:15,058 : INFO : built Dictionary(11 unique tokens: ['numero', 'teksti', 'tulla', 'yksi', 'kaksi']...) from 4 documents (total 15 corpus positions)\n",
      "2019-10-09 10:33:15,058 : INFO : storing corpus in Matrix Market format to /Users/mry/SFI/corpus sample.mm\n",
      "2019-10-09 10:33:15,060 : WARNING : this function is deprecated, use smart_open.open instead\n",
      "2019-10-09 10:33:15,061 : INFO : saving sparse matrix to /Users/mry/SFI/corpus sample.mm\n",
      "2019-10-09 10:33:15,061 : INFO : PROGRESS: saving document #0\n",
      "2019-10-09 10:33:15,062 : INFO : saved 4x11 matrix, density=31.818% (14/44)\n",
      "2019-10-09 10:33:15,063 : INFO : saving MmCorpus index to /Users/mry/SFI/corpus sample.mm.index\n",
      "2019-10-09 10:33:15,063 : WARNING : this function is deprecated, use smart_open.open instead\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample\n",
      "defaultdict(<class 'list'>, {'123': ['nyt teksti tulla teksti numero yksi', 'sitten numero kaksi'], '200': ['tulla kolmas', 'lopuksi viides teksti ja kuudes ja loppu'], '35': ['sitten teksti talous'], '42': ['jos teksti toinen talous', 'ehkä teksti talous vielä']})\n",
      "BEGIN GENSIM - PREPROCESS\n",
      "\n",
      "pre-processed text weeded next\n",
      "\n",
      "weeding tokens, including only those that appear at least 0 times\n",
      "[[b'teksti', b'tulla', b'teksti', b'numero', b'yksi'], [b'sitten', b'numero', b'kaksi'], [b'tulla', b'kolmas'], [b'lopuksi', b'viides', b'teksti', b'kuudes', b'loppu']]\n",
      "CONTINUE GENSIM - DIC AND CORPUS\n",
      "Dictionary(11 unique tokens: ['numero', 'teksti', 'tulla', 'yksi', 'kaksi']...)\n",
      "\n",
      "dictionary of documents: format - (stem, ID)\n",
      "{'numero': 0, 'teksti': 1, 'tulla': 2, 'yksi': 3, 'kaksi': 4, 'sitten': 5, 'kolmas': 6, 'kuudes': 7, 'loppu': 8, 'lopuksi': 9, 'viides': 10}\n",
      "\n",
      "corpus of documents: format - (ID, # of occurrences)\n",
      "[[(0, 1), (1, 2), (2, 1), (3, 1)], [(0, 1), (4, 1), (5, 1)], [(2, 1), (6, 1)], [(1, 1), (7, 1), (8, 1), (9, 1), (10, 1)]]\n",
      "BEGIN SCIKIT LEARN\n",
      "save this one:\n",
      "(4, 13)\n",
      "saved!\n",
      "Extracting features from the data:\n",
      "feature names:\n",
      "['ja', 'kaksi', 'kolmas', 'kuudes', 'loppu', 'lopuksi', 'numero', 'nyt', 'sitten', 'teksti', 'tulla', 'viides', 'yksi']\n",
      "['ja' 'kaksi' 'kolmas' 'kuudes' 'loppu' 'lopuksi' 'numero' 'nyt' 'sitten'\n",
      " 'teksti' 'tulla' 'viides' 'yksi']\n",
      "COMPARE PANDAS\n",
      "                                                   0\n",
      "0    (0, 7)\\t0.4177721783483124\\n  (0, 9)\\t0.6587...\n",
      "1    (0, 6)\\t0.48693426407352264\\n  (0, 8)\\t0.617...\n",
      "2    (0, 10)\\t0.6191302964899972\\n  (0, 2)\\t0.785...\n",
      "3    (0, 9)\\t0.2685092134645423\\n  (0, 5)\\t0.3405...\n"
     ]
    }
   ],
   "source": [
    "## main\n",
    "if __name__ == \"__main__\":\n",
    "    # set working directory\n",
    "    if sampleData:\n",
    "        print(\"sample\")\n",
    "        os.chdir(searchDirectorySample)\n",
    "    else:\n",
    "        printt(\"full data\")\n",
    "        os.chdir(searchDirectory)\n",
    "\n",
    "    ## if starting from scratch, create new dic and corpus file\n",
    "    if not existingLemmaset:\n",
    "\n",
    "        if sampleData:\n",
    "            \n",
    "            ##for pilot data:\n",
    "            \n",
    "            # this small sample worked fine\n",
    "            #texts = [\"nyt teksti tulla teksti numero yksi\", \"sitten numero kaksi\", \"tulla kolmas\", lopuksi viides teksti ja kuudes ja loppu\"]\n",
    "            \n",
    "            # this small sample is not working; here the document number is known (key is thread_id)\n",
    "            #texts = {\n",
    "            #    \"doc1\":\"nyt teksti tulla teksti numero yksi\", \"doc2\":\"sitten numero kaksi\", \"doc3\":\"tulla kolmas\", \"doc4\":\"lopuksi viides teksti ja kuudes ja loppu\"\n",
    "            #}\n",
    "            \n",
    "            # this is not working either; this is the outcome from below, the dictionary created from the full file\n",
    "            # the idea is that one thread has many comments and they are all appended into the value list under the thread_id key\n",
    "            #texts = {\n",
    "            #    \"doc1\":[\"nyt teksti tulla teksti numero yksi\", \"lista arvo toinen\"], \"doc2\":[\"sitten numero kaksi\"], \"doc3\":[\"tulla kolmas\", \"vielä neljäs\"], \"doc4\":[\"lopuksi viides teksti ja kuudes ja loppu\"]\n",
    "            #}\n",
    "            \n",
    "            # for tdfif_vectorizer, corpus should be a list of strings \n",
    "            # ie. I'm hoping to read all comments in one thread into a row in the list. \n",
    "            # these are values from the dictionary so that the dictionary keeps the thread_ids and the original comments intact\n",
    "            \n",
    "            ##for small data:\n",
    "            ## open one file with rows of data including lemmatized sentences in one cell\n",
    "            with open(openFileName, \"r\", encoding=\"utf-8\") as openFile:\n",
    "                documents = csv.DictReader(openFile, delimiter=\",\") #\\t for txt , for csv\n",
    "                texts = defaultdict(list)\n",
    "                for row in documents:\n",
    "                    texts[row[\"tid\"]].append(row[\"lemmas\"])\n",
    "                #print(texts) \n",
    "                \n",
    "        \n",
    "            # I try to do it like this so that we could eg. save the dictionary here and use it later \n",
    "            # in order to create further corpuses where the documents are different combinations of comments\n",
    "            # eg. version two, document is a full subforum and corpus is the collection of these subforums\n",
    "          \n",
    "        else:\n",
    "            # print(\"\\nfollowing files found in directory: \" + str(os.getcwd()))\n",
    "            ## open one file with rows of data including lemmatized sentences in one cell\n",
    "            with open(openFileName, \"r\", encoding=\"utf-8\") as openFile:  # vai latin1\n",
    "                documents = csv.DictReader(openFile, delimiter=\",\") #\\t for txt , for csv\n",
    "                texts = defaultdict(list)\n",
    "                for row in documents:\n",
    "                    texts[row[\"tid\"]].append(row[\"lemmas\"])\n",
    "                #print(texts) \n",
    "            \n",
    "        #after I have the corpus as dictionary, i need to pick all the values from the lists into one string row \n",
    "        #that then becomes the corpus proper\n",
    "              \n",
    "        ## init scikit-learn vectorizer\n",
    "        # returns term-document matrix; equivalent to fit followed by transform, but more effective\n",
    "        # loses the link to document ids, as dict is not ordered. this is why separate dictionary with doc ids as keys seems necessary\n",
    "            \n",
    "        vectorizer = TfidfVectorizer()\n",
    "            \n",
    "        # for tdfif_vectorizer, corpus should be a list of strings \n",
    "        # first trial: each thread = document, all comments concatenated into one string\n",
    "        # second trial: each sub-discussion forum level = document, all comments concatenated into one string\n",
    "            \n",
    "        # this is no working for the actual dictionary data set\n",
    "        #corp = vectorizer.fit_transform(key.values() for key, value in texts.items())\n",
    "        \n",
    "        print(\"BEGIN GENSIM - PREPROCESS\")\n",
    "        # this works for the simplest data set that is only a list\n",
    "        texts = [\"nyt teksti tulla teksti numero yksi\", \"sitten numero kaksi\", \"tulla kolmas\", \"lopuksi viides teksti ja kuudes ja loppu\"]\n",
    "            \n",
    "        # preprocess stop words et al.\n",
    "        processedText = processedText + preProcess(texts)\n",
    "        \n",
    "        # weed the text if you want to minimize the dictionary by removing words that are too rare\n",
    "        # recommended: five words, already does a lot but it depends a bit on the data size\n",
    "        print(\"\\npre-processed text weeded next\")\n",
    "        processedText = weedTokens(processedText)\n",
    "        print(processedText)\n",
    "        \n",
    "        #############\n",
    "        ## here's how i did it earlier with gensim.corpora\n",
    "        # create dictionary and save it for future use as .dic\n",
    "        # do this every time you update your document collection\n",
    "        dic = corpora.Dictionary(processedText)\n",
    "        \n",
    "        print(\"CONTINUE GENSIM - DIC AND CORPUS\")\n",
    "        print(dic)\n",
    "        print(\"\\ndictionary of documents: format - (stem, ID)\")\n",
    "        print(dic.token2id)\n",
    "        #if sampleData:\n",
    "        #    with open(saveFileName, \"w\") as saveFile:\n",
    "        #        writer = csv.DictWriter(saveFile, dic.keys())\n",
    "        #        #writer.writeheader()\n",
    "        #        writer.writerows(dic.token2id)\n",
    "        #else:\n",
    "        #    dic.save(outcomeGensimDir + \"dic.dic\")  # Store the dictionary for future reference\n",
    "        #\n",
    "        #\n",
    "        # create corpus and save it for future use\n",
    "        # this gensim corpus resides fully in RAM memory as plain python list\n",
    "        # with very large datasets this won't do and its preferable to access each document from a file\n",
    "        # this is why I try to replace this simple solution with the scikit dictionary\n",
    "        corpus = [dic.doc2bow(text) for text in processedText]\n",
    "        if sampleData:\n",
    "            corpora.MmCorpus.serialize(outcomeGensimSampleDir + \"corpus sample.mm\", corpus)\n",
    "        else:\n",
    "            corpora.MmCorpus.serialize(outcomeGensimDir + \"corpus.mm\", corpus)\n",
    "        print(\"\\ncorpus of documents: format - (ID, # of occurrences)\")\n",
    "        print(corpus)\n",
    "            \n",
    "        ###############\n",
    "        ## here's how I tried to redo it with scikit learn\n",
    "        print(\"BEGIN SCIKIT LEARN\")\n",
    "        corp = vectorizer.fit_transform(texts)\n",
    "            \n",
    "        # This part you want saved into csv/txt-file; first, document ID (eventually, thread_id), then word ID\n",
    "        # not working\n",
    "        print(\"save this one:\")\n",
    "        a = corp.toarray()\n",
    "        np.savetxt(\"foo.csv\", a, delimiter=\";\")\n",
    "        print(corp.shape)\n",
    "        print(\"saved!\")\n",
    "\n",
    "        # this part you want saved into other csv/txt-file: first, word ID, then word tfidf\n",
    "        # not working\n",
    "        print(\"Extracting features from the data:\")\n",
    "        feature_names = vectorizer.get_feature_names()\n",
    "        print(\"feature names:\")\n",
    "        print(feature_names)\n",
    "        feature_names = np.asarray(feature_names)\n",
    "            \n",
    "        print(feature_names)\n",
    "        \n",
    "        print(\"COMPARE PANDAS\")\n",
    "        # trying things out\n",
    "        df2 = pd.DataFrame(corp)\n",
    "        print(df2.head())\n",
    "        #sparse to dense version (don't use for large vocabulary)\n",
    "        #corpus_index = [n for n in texts]\n",
    "        #dense = corp.todense()\n",
    "        #denselist = dense.tolist()\n",
    "        #df = pd.DataFrame(denselist, columns = feature_names, index = corpus_index)\n",
    "        #print(df.head())\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
